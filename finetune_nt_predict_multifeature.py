#!/usr/bin/env python3
# finetune_nt_predict_multifeature.py
"""
å¤šç‰¹å¾ç‰ˆæœ¬çš„ Nucleotide Transformer é¢„æµ‹è„šæœ¬
æ”¯æŒåºåˆ—ç‰¹å¾ + æ•°å€¼ç‰¹å¾çš„è”åˆé¢„æµ‹
"""

import argparse
import os
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoModel, AutoTokenizer, AutoConfig
from tqdm.auto import tqdm
import numpy as np

# ---------------------
# æ¨¡å‹å®šä¹‰ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
# ---------------------
class MultiFeatureNTClassificationModel(nn.Module):
    def __init__(self, backbone, num_numerical_features, num_labels=2, dropout=0.3):
        super().__init__()
        self.backbone = backbone
        
        # è·å–éšè—å±‚ç»´åº¦
        if hasattr(backbone, 'config') and hasattr(backbone.config, 'hidden_size'):
            hidden_size = backbone.config.hidden_size
        elif hasattr(backbone, 'config') and hasattr(backbone.config, 'd_model'):
            hidden_size = backbone.config.d_model
        else:
            try:
                sample_param = next(backbone.parameters())
                hidden_size = sample_param.shape[0]
            except StopIteration:
                hidden_size = 1024
        self.hidden_size = hidden_size
        
        # æ•°å€¼ç‰¹å¾å¤„ç†ç½‘ç»œ
        self.numerical_processor = nn.Sequential(
            nn.Linear(num_numerical_features, 128),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # èåˆåˆ†ç±»å™¨ï¼ˆåºåˆ—ç‰¹å¾ + æ•°å€¼ç‰¹å¾ï¼‰
        combined_dim = hidden_size + 64
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(combined_dim, 512),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(512, 128),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(128, num_labels)
        )

    def forward(self, input_ids=None, attention_mask=None, numerical_features=None, **kwargs):
        # å¤„ç†åºåˆ—ç‰¹å¾
        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask, **kwargs)
        
        if hasattr(outputs, 'last_hidden_state'):
            last_hidden = outputs.last_hidden_state
        elif isinstance(outputs, (tuple, list)) and len(outputs) > 0:
            last_hidden = outputs[0]
        else:
            raise RuntimeError("æ— æ³•è·å– backbone çš„éšè—çŠ¶æ€")
        
        # åºåˆ—ç‰¹å¾æ± åŒ–ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
        avg_pool = last_hidden.mean(dim=1)
        max_pool = last_hidden.max(dim=1)[0]
        seq_features = avg_pool + max_pool
        
        # å¤„ç†æ•°å€¼ç‰¹å¾
        if numerical_features is not None:
            numerical_features = self.numerical_processor(numerical_features)
            # èåˆç‰¹å¾
            combined_features = torch.cat([seq_features, numerical_features], dim=1)
        else:
            combined_features = seq_features
        
        logits = self.classifier(combined_features)
        return logits

# ---------------------
# æ•°æ®å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
# ---------------------
class MultiFeatureSeqDataset(torch.utils.data.Dataset):
    def __init__(self, sequences, features, tokenizer, max_length=512, feature_dim=6):
        self.seqs = sequences
        self.features = features
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.feature_dim = feature_dim

    def __len__(self):
        return len(self.seqs)

    def __getitem__(self, idx):
        seq = self.seqs[idx]
        feature_vec = self.features[idx]
        
        # Tokenize åºåˆ—
        enc = self.tokenizer(
            seq,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )
        
        item = {k: v.squeeze(0) for k, v in enc.items()}
        item["numerical_features"] = torch.tensor(feature_vec, dtype=torch.float)
        
        return item

def read_csv_multifeature(path: str):
    """è¯»å–CSVæ–‡ä»¶ï¼Œè¿”å›åºåˆ—å’Œæ•°å€¼ç‰¹å¾åˆ—è¡¨"""
    seqs, features = [], []
    
    try:
        df = pd.read_csv(path)
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        if 'Off' not in df.columns:
            raise ValueError("CSVæ–‡ä»¶å¿…é¡»åŒ…å« 'Off' åˆ—")
        
        # åºåˆ—åˆ—
        seqs = df['Off'].astype(str).tolist()
        
        # ç‰¹å¾åˆ—
        feature_columns = ['Epi_satics', 'CFD_score', 'CCTop_Score', 'Moreno_Score', 'CROPIT_Score', 'MIT_Score']
        available_columns = []
        for col in feature_columns:
            if col in df.columns:
                available_columns.append(col)
            else:
                print(f"è­¦å‘Š: ç‰¹å¾åˆ— {col} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        
        # æå–æ•°å€¼ç‰¹å¾
        if available_columns:
            features = df[available_columns].values.tolist()
        else:
            features = [[0] * 6 for _ in range(len(seqs))]  # é»˜è®¤ç‰¹å¾
        
        print(f"ä½¿ç”¨çš„ç‰¹å¾åˆ—: {available_columns}")
        print(f"ç‰¹å¾ç»´åº¦: {len(available_columns)}")
        
    except Exception as e:
        print(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
        raise
    
    return seqs, features, df

# ---------------------
# æœ¬åœ°æ¨¡å‹åŠ è½½å‡½æ•°
# ---------------------
def load_model_and_tokenizer_locally(model_path, device):
    """ä»æœ¬åœ°ç›®å½•åŠ è½½æ¨¡å‹å’Œtokenizer"""
    print("ğŸ”§ æ­£åœ¨ä»æœ¬åœ°åŠ è½½æ¨¡å‹å’Œtokenizer...")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯HuggingFaceæ¨¡å‹ç›®å½•
    if os.path.isdir(model_path):
        model_dir = model_path
    else:
        checkpoint_dir = os.path.dirname(model_path)
        possible_dirs = [
            os.path.join(checkpoint_dir, "model"),
            checkpoint_dir,
            os.path.dirname(checkpoint_dir)
        ]
        
        model_dir = None
        for dir_path in possible_dirs:
            if os.path.exists(os.path.join(dir_path, "config.json")):
                model_dir = dir_path
                break
        
        if model_dir is None:
            print("âŒ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹æ–‡ä»¶")
            return None, None
    
    # åŠ è½½tokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_dir,
            local_files_only=True,
            trust_remote_code=True
        )
        print("âœ… Tokenizeræœ¬åœ°åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Tokenizeræœ¬åœ°åŠ è½½å¤±è´¥: {e}")
        return None, None
    
    # åŠ è½½æ¨¡å‹é…ç½®ä¸backbone
    try:
        config = AutoConfig.from_pretrained(model_dir, local_files_only=True)
        backbone = AutoModel.from_pretrained(
            model_dir,
            config=config,
            local_files_only=True,
            trust_remote_code=True
        )
        print("âœ… æ¨¡å‹æœ¬åœ°åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹æœ¬åœ°åŠ è½½å¤±è´¥: {e}")
        return None, None
    
    return backbone, tokenizer

def download_model_locally(model_name="InstaDeepAI/nucleotide-transformer-500m-1000g", local_dir="./local_models"):
    """ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç›®å½•"""
    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½æ¨¡å‹ {model_name} åˆ°æœ¬åœ°ç›®å½• {local_dir}...")
    
    os.makedirs(local_dir, exist_ok=True)
    model_dir = os.path.join(local_dir, model_name.split('/')[-1])
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            cache_dir=local_dir
        )
        tokenizer.save_pretrained(model_dir)
        print("âœ… Tokenizerä¸‹è½½å¹¶ä¿å­˜æˆåŠŸ")
        
        model = AutoModel.from_pretrained(
            model_name,
            trust_remote_code=True,
            cache_dir=local_dir
        )
        model.save_pretrained(model_dir)
        print("âœ… æ¨¡å‹ä¸‹è½½å¹¶ä¿å­˜æˆåŠŸ")
        
        print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_dir}")
        return model_dir
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return None

# ---------------------
# æ¨¡å‹åŠ è½½
# ---------------------
def load_trained_model(checkpoint_path, local_model_dir, device, feature_dim=6):
    """åŠ è½½è®­ç»ƒå¥½çš„å¤šç‰¹å¾æ¨¡å‹"""
    print(f"åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # åŠ è½½backboneï¼ˆæœ¬åœ°ï¼‰
    backbone, tokenizer = load_model_and_tokenizer_locally(local_model_dir, device)
    if backbone is None:
        return None, None
    
    # åˆ›å»ºå¤šç‰¹å¾æ¨¡å‹æ¶æ„
    model = MultiFeatureNTClassificationModel(
        backbone, 
        num_numerical_features=feature_dim, 
        num_labels=2
    )
    
    # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        model.load_state_dict(checkpoint["model_state_dict"])
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥ç‰¹å¾ç»´åº¦æ˜¯å¦åŒ¹é…
        saved_feature_dim = checkpoint.get("feature_dim", feature_dim)
        if saved_feature_dim != feature_dim:
            print(f"âš ï¸ è­¦å‘Š: ä¿å­˜çš„ç‰¹å¾ç»´åº¦({saved_feature_dim})ä¸å½“å‰ç‰¹å¾ç»´åº¦({feature_dim})ä¸åŒ¹é…")
            
    except Exception as e:
        print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
        return None, None

    # ç¡®ä¿æ¨¡å‹å„éƒ¨åˆ†dtypeä¸€è‡´
    backbone_dtype = None
    for p in model.backbone.parameters():
        backbone_dtype = p.dtype
        break
    if backbone_dtype is None:
        backbone_dtype = torch.float32

    try:
        model.numerical_processor.to(dtype=backbone_dtype)
        model.classifier.to(dtype=backbone_dtype)
    except Exception as e:
        print(f"âš ï¸ è½¬æ¢dtypeæ—¶é‡åˆ°é—®é¢˜: {e}")

    # å°†æ•´ä¸ªæ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
    try:
        model.to(device)
    except Exception as e:
        print(f"âŒ å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡ {device} å¤±è´¥: {e}")
        return None, None

    model.eval()
    return model, tokenizer

# ---------------------
# é¢„æµ‹å‡½æ•°
# ---------------------
def predict(model, dataloader, device):
    """è¿›è¡Œå¤šç‰¹å¾é¢„æµ‹"""
    model.eval()
    all_probs = []
    all_logits = []
    all_predictions = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="é¢„æµ‹ä¸­"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            numerical_features = batch["numerical_features"].to(device)
            
            logits = model(
                input_ids=input_ids, 
                attention_mask=attention_mask,
                numerical_features=numerical_features
            )
            probs = torch.softmax(logits, dim=-1)
            
            all_probs.extend(probs.cpu().numpy())
            all_logits.extend(logits.cpu().numpy())
            all_predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())
    
    return all_probs, all_logits, all_predictions

# ---------------------
# ä¸»å‡½æ•°
# ---------------------
def main():
    parser = argparse.ArgumentParser(description="å¤šç‰¹å¾ Nucleotide Transformer é¢„æµ‹è„šæœ¬")
    parser.add_argument("--checkpoint", type=str, required=True, 
                       help="è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--input_csv", type=str, required=True,
                       help="è¾“å…¥æ•°æ®CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_csv", type=str, required=True,
                       help="é¢„æµ‹ç»“æœè¾“å‡ºCSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--local_model_dir", type=str, 
                       default="./local_models/nucleotide-transformer-500m-1000g",
                       help="æœ¬åœ°æ¨¡å‹ç›®å½•è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=16,
                       help="é¢„æµ‹æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--max_length", type=int, default=512,
                       help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--device", type=str, default="cuda",
                       help="æ¨ç†è®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--download_model", action="store_true",
                       help="å¦‚æœæœ¬åœ°æ²¡æœ‰æ¨¡å‹ï¼Œå…ˆä¸‹è½½æ¨¡å‹")
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device if (torch.cuda.is_available() and 'cuda' in args.device) else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.checkpoint):
        print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
        return
    
    if not os.path.exists(args.input_csv):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_csv}")
        return
    
    # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.local_model_dir) and args.download_model:
        print("æœ¬åœ°æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
        model_dir = download_model_locally(local_dir=os.path.dirname(args.local_model_dir))
        if model_dir is None:
            return
    elif not os.path.exists(args.local_model_dir):
        print(f"âŒ æœ¬åœ°æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {args.local_model_dir}")
        print("è¯·ä½¿ç”¨ --download_model å‚æ•°è‡ªåŠ¨ä¸‹è½½")
        return
    
    # åŠ è½½æ•°æ®å¹¶è·å–ç‰¹å¾ç»´åº¦
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    sequences, features, original_df = read_csv_multifeature(args.input_csv)
    feature_dim = len(features[0]) if features else 6
    print(f"åŠ è½½äº† {len(sequences)} æ¡åºåˆ—ï¼Œç‰¹å¾ç»´åº¦: {feature_dim}")
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model, tokenizer = load_trained_model(args.checkpoint, args.local_model_dir, device, feature_dim)
    if model is None:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
        return
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = MultiFeatureSeqDataset(sequences, features, tokenizer, args.max_length, feature_dim)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)
    
    # è¿›è¡Œé¢„æµ‹
    print("å¼€å§‹å¤šç‰¹å¾é¢„æµ‹...")
    probabilities, logits, predictions = predict(model, dataloader, device)
    
    # å‡†å¤‡è¾“å‡ºç»“æœ
    results_df = original_df.copy()
    
    # æ·»åŠ é¢„æµ‹ç»“æœ
    results_df['prediction'] = predictions
    results_df['probability_class_0'] = [prob[0] for prob in probabilities]
    results_df['probability_class_1'] = [prob[1] for prob in probabilities]
    results_df['confidence'] = np.max(probabilities, axis=1)
    
    # æ·»åŠ é¢„æµ‹æ ‡ç­¾
    results_df['predicted_label'] = results_df['prediction'].map({0: 'negative', 1: 'positive'})
    
    # ä¿å­˜ç»“æœ
    results_df.to_csv(args.output_csv, index=False)
    print(f"âœ… é¢„æµ‹å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {args.output_csv}")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(results_df)}")
    print(f"   é¢„æµ‹ä¸ºæ­£ç±»çš„æ ·æœ¬æ•°: {sum(predictions)}")
    print(f"   é¢„æµ‹ä¸ºè´Ÿç±»çš„æ ·æœ¬æ•°: {len(predictions) - sum(predictions)}")
    print(f"   å¹³å‡ç½®ä¿¡åº¦: {np.mean(results_df['confidence']):.4f}")

if __name__ == "__main__":
    main()